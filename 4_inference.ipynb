{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# NOTE: restart the kernel before running every section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Local inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch, sagemaker\n",
    "from sagemaker.s3 import S3Downloader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.session.Session()\n",
    "\n",
    "training_job_name = \"fsdp-codestral-2024-06-17-05-45-33-367\"\n",
    "estimator = sagemaker.estimator.Estimator.attach(training_job_name)\n",
    "model_s3_path = estimator.model_data[\"S3DataSource\"][\"S3Uri\"]\n",
    "model_local_path = \"/home/ubuntu/finetune-llms-on-aws/practise-fsdp/sft_cache/sm_model\"\n",
    "\n",
    "print(f\"-\"*25)\n",
    "print(f\"model_s3_path: {model_s3_path}\")\n",
    "print(f\"model_local_path: {model_local_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p $model_local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = S3Downloader.download(\n",
    "#     s3_uri=model_s3_path,                    # S3 URI where the trained model is located\n",
    "#     local_path=model_local_path,             # local path where *.targ.gz is saved\n",
    "#     sagemaker_session=session                # SageMaker session used for training the model\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if tokenizer wasn't saved\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mistral-community/Codestral-22B-v0.1\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"right\"\n",
    "\n",
    "# tokenizer.save_pretrained(model_local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 sync $model_local_path $model_s3_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_local_path, trust_remote_code=True,\n",
    "    # \"mistral-community/Codestral-22B-v0.1\"\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_local_path,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Recommend the top 3 code generation language models to use for Rust\"\n",
    "prompt = \"How to solve high leverage AI research problems ? And give examples where AI research helped humanity make leaps of progress.\"\n",
    "\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs, max_new_tokens=1000, do_sample=True\n",
    ")\n",
    "print(tokenizer.batch_decode(generated_ids)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Local inference with vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install vllm ray\n",
    "from vllm import LLM, SamplingParams\n",
    "import gc, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"mistral-community/Codestral-22B-v0.1\" # original\n",
    "model_id = \"/home/ubuntu/finetune-llms-on-aws/practise-fsdp/sft_cache/sm_model\" # finetuned\n",
    "\n",
    "print(f\"model_id: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(model_id, tensor_parallel_size=4, dtype=\"bfloat16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"Recommend the top 3 code generation language models to use for Rust\"\n",
    "prompt = \"How to solve high leverage AI research problems ? And give examples where AI research helped humanity make leaps of progress.\"\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=1024)\n",
    "\n",
    "output = llm.generate(prompt, sampling_params)\n",
    "print(output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Host sagemaker inference endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, json, boto3, sagemaker, jinja2, pathlib\n",
    "from sagemaker import Model, image_uris, serializers, deserializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "session = sagemaker.session.Session()\n",
    "region = session._region_name\n",
    "jinja_env = jinja2.Environment()\n",
    "\n",
    "# training_job_name = \"fsdp-codestral-2024-06-17-05-45-33-367\"\n",
    "# estimator = sagemaker.estimator.Estimator.attach(training_job_name)\n",
    "# model_s3_path = estimator.model_data[\"S3DataSource\"][\"S3Uri\"]\n",
    "model_s3_path = \"s3://research-agi/mistral-community-codestral-22b-v0x1/runs/fsdp-codestral-2024-06-17-05-45-33-367/output/model/\"\n",
    "s3_code_prefix = \"djl_inference\"\n",
    "tar_file = \"djl_inference_code.tar.gz\"\n",
    "\n",
    "print(f'role: {role} region: {region}')\n",
    "print(f\"model_s3_path: {model_s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p djl_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile djl_inference/serving.properties\n",
    "engine=Python\n",
    "option.model_id={{s3url}}\n",
    "option.rolling_batch=vllm\n",
    "option.dtype=bf16\n",
    "option.tensor_parallel_degree=4\n",
    "option.max_rolling_batch_size=1\n",
    "option.model_loading_timeout=1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = jinja_env.from_string(\n",
    "    pathlib.Path(\"djl_inference/serving.properties\").open().read()\n",
    ")\n",
    "\n",
    "pathlib.Path(\"djl_inference/serving.properties\").open(\"w\").write(\n",
    "    template.render(s3url=model_s3_path)\n",
    ")\n",
    "\n",
    "!pygmentize djl_inference/serving.properties | cat -n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "tar czvf djl_inference_code.tar.gz djl_inference/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = session.default_bucket()  # bucket to house artifacts\n",
    "code_artifact = session.upload_data(tar_file, bucket, s3_code_prefix)\n",
    "\n",
    "print(f\"s3 code uploaded to: {code_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "rm -rf djl_inference\n",
    "rm -rf djl_inference_code.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-deepspeed\", \n",
    "    region=session.boto_session.region_name, \n",
    "    version=\"0.27.0\"\n",
    ")\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"codestral-vllm\")\n",
    "\n",
    "print(f\"endpoint_name: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = Model(image_uri=image_uri, model_data=code_artifact, role=role)\n",
    "\n",
    "model.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type=instance_type,\n",
    "   endpoint_name=endpoint_name,\n",
    "   container_startup_health_check_timeout=1800,\n",
    "   # volume_size=300, # comment for g5\n",
    "   endpoint_logging=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=session,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    "    deserializer=deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "prompt = \"How to solve high leverage AI research problems ? And give examples where AI research helped humanity make leaps of progress.\"\n",
    "\n",
    "res = predictor.predict(\n",
    "    {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\":128, \n",
    "            \"do_sample\":\"true\",\n",
    "            # \"model_name\": \"mistral-community/Codestral-22B-v0.1\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print(res[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
