{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# NOTE: restart the kernel before running every section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Local inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch, sagemaker\n",
    "from sagemaker.s3 import S3Downloader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.session.Session()\n",
    "\n",
    "training_job_name = \"fsdp-codestral-2024-06-17-05-45-33-367\"\n",
    "estimator = sagemaker.estimator.Estimator.attach(training_job_name)\n",
    "model_s3_path = estimator.model_data[\"S3DataSource\"][\"S3Uri\"]\n",
    "model_local_path = \"/home/ubuntu/finetune-llms-on-aws/practise-fsdp/sft_cache/sm_model\"\n",
    "\n",
    "print(f\"-\"*25)\n",
    "print(f\"model_s3_path: {model_s3_path}\")\n",
    "print(f\"model_local_path: {model_local_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p $model_local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = S3Downloader.download(\n",
    "#     s3_uri=model_s3_path,                    # S3 URI where the trained model is located\n",
    "#     local_path=model_local_path,             # local path where *.targ.gz is saved\n",
    "#     sagemaker_session=session                # SageMaker session used for training the model\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if tokenizer wasn't saved\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mistral-community/Codestral-22B-v0.1\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"right\"\n",
    "\n",
    "# tokenizer.save_pretrained(model_local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 sync $model_local_path $model_s3_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_local_path, trust_remote_code=True,\n",
    "    # \"mistral-community/Codestral-22B-v0.1\"\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_local_path,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Recommend the top 3 code generation language models to use for Rust\"\n",
    "prompt = \"How to solve high leverage AI research problems ? And give examples where AI research helped humanity make leaps of progress.\"\n",
    "\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs, max_new_tokens=1000, do_sample=True\n",
    ")\n",
    "print(tokenizer.batch_decode(generated_ids)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Local inference with vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install vllm ray\n",
    "from vllm import LLM, SamplingParams\n",
    "import gc, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"mistral-community/Codestral-22B-v0.1\" # original\n",
    "model_id = \"/home/ubuntu/finetune-llms-on-aws/practise-fsdp/sft_cache/sm_model\" # finetuned\n",
    "\n",
    "print(f\"model_id: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(model_id, tensor_parallel_size=4, dtype=\"bfloat16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"Recommend the top 3 code generation language models to use for Rust\"\n",
    "prompt = \"How to solve high leverage AI research problems ? And give examples where AI research helped humanity make leaps of progress.\"\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=1024)\n",
    "\n",
    "output = llm.generate(prompt, sampling_params)\n",
    "print(output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Host sagemaker inference endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ubuntu/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import io, json, boto3, sagemaker, jinja2, pathlib\n",
    "from sagemaker import Model, image_uris, serializers, deserializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role: arn:aws:iam::324622400514:role/ec2-vscode-role region: us-east-1\n",
      "model_s3_path: s3://research-agi/mistral-community-codestral-22b-v0x1/runs/fsdp-codestral-2024-06-17-05-45-33-367/output/model/\n"
     ]
    }
   ],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "session = sagemaker.session.Session()\n",
    "region = session._region_name\n",
    "jinja_env = jinja2.Environment()\n",
    "\n",
    "# training_job_name = \"fsdp-codestral-2024-06-17-05-45-33-367\"\n",
    "# estimator = sagemaker.estimator.Estimator.attach(training_job_name)\n",
    "# model_s3_path = estimator.model_data[\"S3DataSource\"][\"S3Uri\"]\n",
    "model_s3_path = \"s3://research-agi/mistral-community-codestral-22b-v0x1/runs/fsdp-codestral-2024-06-17-05-45-33-367/output/model/\"\n",
    "s3_code_prefix = \"djl_inference\"\n",
    "tar_file = \"djl_inference_code.tar.gz\"\n",
    "\n",
    "print(f'role: {role} region: {region}')\n",
    "print(f\"model_s3_path: {model_s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p djl_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing djl_inference/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile djl_inference/serving.properties\n",
    "engine=Python\n",
    "option.model_id={{s3url}}\n",
    "option.rolling_batch=vllm\n",
    "option.dtype=bf16\n",
    "option.tensor_parallel_degree=4\n",
    "option.max_rolling_batch_size=1\n",
    "option.model_loading_timeout=1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\t\u001b[36mengine\u001b[39;49;00m=\u001b[33mPython\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     2\t\u001b[36moption.model_id\u001b[39;49;00m=\u001b[33ms3://research-agi/mistral-community-codestral-22b-v0x1/runs/fsdp-codestral-2024-06-17-05-45-33-367/output/model/\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     3\t\u001b[36moption.rolling_batch\u001b[39;49;00m=\u001b[33mvllm\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     4\t\u001b[36moption.dtype\u001b[39;49;00m=\u001b[33mbf16\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     5\t\u001b[36moption.tensor_parallel_degree\u001b[39;49;00m=\u001b[33m4\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     6\t\u001b[36moption.max_rolling_batch_size\u001b[39;49;00m=\u001b[33m1\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     7\t\u001b[36moption.model_loading_timeout\u001b[39;49;00m=\u001b[33m1800\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "template = jinja_env.from_string(\n",
    "    pathlib.Path(\"djl_inference/serving.properties\").open().read()\n",
    ")\n",
    "\n",
    "pathlib.Path(\"djl_inference/serving.properties\").open(\"w\").write(\n",
    "    template.render(s3url=model_s3_path)\n",
    ")\n",
    "\n",
    "!pygmentize djl_inference/serving.properties | cat -n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "djl_inference/\n",
      "djl_inference/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "tar czvf djl_inference_code.tar.gz djl_inference/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3 code uploaded to: s3://sagemaker-us-east-1-324622400514/djl_inference/djl_inference_code.tar.gz\n"
     ]
    }
   ],
   "source": [
    "bucket = session.default_bucket()  # bucket to house artifacts\n",
    "code_artifact = session.upload_data(tar_file, bucket, s3_code_prefix)\n",
    "\n",
    "print(f\"s3 code uploaded to: {code_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "rm -rf djl_inference\n",
    "rm -rf djl_inference_code.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint_name: codestral-vllm-2024-06-18-16-52-35-354\n"
     ]
    }
   ],
   "source": [
    "image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-deepspeed\", \n",
    "    region=session.boto_session.region_name, \n",
    "    version=\"0.27.0\"\n",
    ")\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"codestral-vllm\")\n",
    "\n",
    "print(f\"endpoint_name: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 726 ms, sys: 43.1 ms, total: 769 ms\n",
      "Wall time: 8min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = Model(image_uri=image_uri, model_data=code_artifact, role=role)\n",
    "\n",
    "model.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type=instance_type,\n",
    "   endpoint_name=endpoint_name,\n",
    "   container_startup_health_check_timeout=1800,\n",
    "   # volume_size=300, # comment for g5\n",
    "   endpoint_logging=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'codestral-vllm-2024-06-18-16-52-35-354'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=session,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    "    deserializer=deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " You must first understand the problem statement defined within problem_description tags and generate code that will pass all the tests:\n",
      "\n",
      "<problem_description>\n",
      "The problem is about solving high leverage AI research problems using a systematic approach. The task requires defining the problem, identifying subproblems, and generating and optimizing ideas to solve the problem efficiently.\n",
      "\n",
      "To illustrate the concept, the task provides two examples, one being the sports match prediction problem and the other being the computer chess. For each example, the task describes the steps taken to approach the problem, generate ideas, and solve it using AI techniques\n",
      "CPU times: user 3.01 ms, sys: 297 µs, total: 3.3 ms\n",
      "Wall time: 4.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "prompt = \"How to solve high leverage AI research problems ? And give examples where AI research helped humanity make leaps of progress.\"\n",
    "\n",
    "res = predictor.predict(\n",
    "    {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\":128, \n",
    "            \"do_sample\":\"true\",\n",
    "            # \"model_name\": \"mistral-community/Codestral-22B-v0.1\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print(res[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
