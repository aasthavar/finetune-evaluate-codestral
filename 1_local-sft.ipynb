{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 1. Usecase and task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "- the competitive programming code generation problem can be viewed as a sequence-to-sequence translation task\n",
    "- given a problem description 'X' in natural language, produce a corresponding solution 'Y' in a programming language. \n",
    "- The metric used for evaluation is \"percentage of problems solved using 'n' submissions from 'k' samples per problem\", denoted as 'n@k'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup development environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --quiet \\\n",
    "    \"torch==2.3.0\" \\\n",
    "    tensorboard\n",
    "\n",
    "! pip install --upgrade --quiet \\\n",
    "    \"transformers==4.41.2\" \\\n",
    "    \"accelerate==0.30.1\" \\\n",
    "    \"datasets==2.19.1\" \\\n",
    "    \"peft==0.11.1\" \\\n",
    "    \"bitsandbytes==0.43.1\" \\\n",
    "    \"trl==0.8.6\" \\\n",
    "    \"evaluate==0.4.2\" \\\n",
    "    huggingface_hub huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\n",
    "\n",
    "# on a multi-gpu machine\n",
    "! pip install flash-attn --no-build-isolation --quiet\n",
    "\n",
    "# NOTE: use when 'Hardware not supported for Flash Attention'\n",
    "# on a single gpu or only cpu machine \n",
    "# ! pip install ninja packaging --quiet\n",
    "# ! MAX_JOBS=4 pip install flash-attn --no-build- --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    " \n",
    "login(\n",
    "  token=\"\", # ADD YOUR TOKEN HERE\n",
    "  add_to_git_credential=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from pprint import pprint\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 1: download dataset from hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"deepmind/code_contests\"\n",
    "\n",
    "dataset = load_dataset(dataset_id, split=\"train[:1%]\")\n",
    "# dataset = load_dataset(dataset_id, split=\"test\") # uncomment when want to perform eval inference\n",
    "print(f\"len(dataset): {len(dataset)}\\nfeatures:\")\n",
    "pprint(dataset.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 2. apply filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_python_solutions(sample):\n",
    "    df = pd.DataFrame(sample[\"solutions\"])\n",
    "    df_python = df[(df.language==3) | (df.language==1)]\n",
    "    return df_python.shape[0]\n",
    "\n",
    "# get instances with 2000+ rating and contains python lang solutions\n",
    "dataset = dataset.filter(lambda sample: (sample[\"cf_rating\"] >= 2000) & (count_python_solutions(sample) >= 1))\n",
    "print(f\"len(dataset): {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 3: augment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: rethink how to do this: next idea to explore: flatten solutions -> and then map perhaps\n",
    "def augment_dataset(dataset):\n",
    "    df = dataset.to_pandas()\n",
    "    aug_rows = []\n",
    "    for i, item in df.iterrows():\n",
    "        for j, soln in enumerate(item[\"solutions\"][\"solution\"]):\n",
    "            language = item[\"solutions\"][\"language\"][j]\n",
    "            if (language==3 or language==1): # python3 or python2\n",
    "                item_new = item.copy(deep=True)\n",
    "                item_new[\"python_solution\"] = soln\n",
    "                item_new.drop('solutions', inplace=True)\n",
    "                aug_rows.append(item_new)\n",
    "    aug_df = pd.DataFrame(aug_rows)\n",
    "    aug_ds = Dataset.from_pandas(aug_df)\n",
    "    return aug_ds\n",
    "\n",
    "# augment dataset: 1{1_problem + n_solutions} to n{1_problem + 1_solution}\n",
    "dataset = augment_dataset(dataset) \n",
    "print(f\"len(dataset): {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 4. apply instruct prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_instruct_template = \"[INST]{instruction}[/INST]\"\n",
    "\n",
    "system_prompt = \"\"\"You are a helpful code assistant. Ensure any code you provide can be executed with all required imports and variables defined. \n",
    "\n",
    "You must understand problem statement defined within problem_description tags and generate code that will pass all the tests:\n",
    "<context>\n",
    "{description}\n",
    "{tests}\n",
    "</context>\n",
    "\n",
    "Begin!\n",
    "You must generate only code with all required imports within <answer> XML tags.\"\"\"\n",
    "\n",
    "human_prompt = \"\"\"Generate code in Python.\"\"\"\n",
    "\n",
    "assistant_prompt = \"\"\"<answer>\n",
    "{code}\n",
    "</answer>\"\"\"\n",
    "\n",
    "# tests_item_format = \"\"\"<item idx={idx}>\n",
    "# Input:\n",
    "# {inputs}\n",
    "# Output:\n",
    "# {outputs}\n",
    "# </item>\n",
    "# \"\"\"\n",
    "\n",
    "tests_item_format = \"\"\"Input:\n",
    "{inputs}\n",
    "Output:\n",
    "{outputs}\n",
    "\"\"\"\n",
    "\n",
    "def format_dataset(sample):\n",
    "    # a. construct prompt\n",
    "    tests = sample[\"private_tests\"]\n",
    "    tests_formatted = \"\\n\".join([\n",
    "        tests_item_format.format(idx=idx, inputs=i.strip(), outputs=o.strip()) \n",
    "        for idx, (i,o) in enumerate(\n",
    "            zip(tests[\"input\"], tests[\"output\"])\n",
    "        )\n",
    "    ])\n",
    "    system_message = system_prompt.format(\n",
    "        description=sample[\"description\"].replace(\"<image>\", \"IMAGE\"),\n",
    "        tests=tests_formatted\n",
    "    )\n",
    "    human_message = human_prompt\n",
    "    instruction = f\"{system_message}\\n\\n{human_message}\"\n",
    "    prompt = mistral_instruct_template.format(instruction=instruction)\n",
    "    \n",
    "    # b. construct completion\n",
    "    completion = assistant_prompt.format(\n",
    "        code=sample[\"python_solution\"]\n",
    "    )\n",
    "   \n",
    "    # c. instruction format\n",
    "    sample[\"prompt\"] = prompt\n",
    "    sample[\"completion\"] = completion\n",
    "    return sample\n",
    "\n",
    "# convert dataset to instruct prompt template\n",
    "columns_to_remove = list(dataset.features)\n",
    "print(f\"columns_to_remove: {columns_to_remove}\")\n",
    "dataset = dataset.map(format_dataset, remove_columns=columns_to_remove, batched=False)\n",
    "print(f\"len(dataset): {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print random sample\n",
    "pprint(dataset[randint(0, len(dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Finetune LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 1: initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### model\n",
    "model_id = \"mistral-community/Codestral-22B-v0.1\"\n",
    "\n",
    "### qlora related\n",
    "r = 64\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "target_modules = [ \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "task_type = \"CAUSAL_LM\"\n",
    "\n",
    "### bitsandbytes related\n",
    "load_in_4bit=True\n",
    "bnb_4bit_use_double_quant=True\n",
    "bnb_4bit_quant_type=\"nf4\"\n",
    "bnb_4bit_compute_dtype=\"bfloat16\"\n",
    "\n",
    "\n",
    "### training related\n",
    "output_dir = \"/home/ubuntu/finetune-llms-on-aws/practise-fsdp/sft_cache/checkpoints\" # prexisting folder path\n",
    "save_model_dir = \"/home/ubuntu/finetune-llms-on-aws/practise-fsdp/sft_cache/model/\"  # prexisting folder path\n",
    "offload_folder = \"/home/ubuntu/finetune-llms-on-aws/practise-fsdp/sft_cache/offload\" # prexisting folder path\n",
    "logging_dir=f\"{output_dir}/logs\"\n",
    "\n",
    "num_train_epochs = 1\n",
    "max_steps = 100 # mumber of training steps (overrides num_train_epochs)\n",
    "\n",
    "per_device_train_batch_size = 1\n",
    "per_device_eval_batch_size = 1\n",
    "gradient_accumulation_steps = 1\n",
    "gradient_checkpointing = True\n",
    "\n",
    "bf16 = True\n",
    "fp16 = False\n",
    "\n",
    "max_grad_norm = 0.3\n",
    "weight_decay = 0.001\n",
    "# optim = \"paged_adamw_32bit\"\n",
    "optim = \"adamw_torch\"\n",
    "\n",
    "learning_rate = 2e-4\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "save_strategy = \"no\"\n",
    "logging_steps = 25\n",
    "logging_strategy = \"steps\"\n",
    "group_by_length = True\n",
    "\n",
    "max_seq_length = 4096\n",
    "packing = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 2: instantiate tokenizer and quantized model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_use_double_quant=bnb_4bit_use_double_quant,\n",
    "    bnb_4bit_compute_dtype=getattr(torch, bnb_4bit_compute_dtype),\n",
    ")\n",
    "# define model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    use_cache=False if gradient_checkpointing else True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1 # num_of_gpus\n",
    "model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 3: define lora config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, bnb.nn.Linear4bit):\n",
    "            names = name.split(\".\")\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if \"lm_head\" in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove(\"lm_head\")\n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "# get lora target modules\n",
    "modules = find_all_linear_names(model)\n",
    "print(modules) # NOTE: update target_modules with these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    target_modules=target_modules,\n",
    "    bias=\"none\",\n",
    "    task_type=task_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 4: define training args, collator, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set training arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    max_steps=max_steps,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    weight_decay=weight_decay,    \n",
    "    optim=optim,\n",
    "    learning_rate=learning_rate,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    save_strategy=save_strategy,\n",
    "    logging_steps=logging_steps,\n",
    "    logging_strategy=logging_strategy,\n",
    "    group_by_length=group_by_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkout for more info: Train on completions only https://huggingface.co/docs/trl/en/sft_trainer\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['prompt'])):\n",
    "        text = f\"{example['prompt'][i]}\\n\\n ### Answer: {example['completion'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=\"### Answer:\", \n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize sft trainer\n",
    "trainer = SFTTrainer(\n",
    "    args=training_arguments,\n",
    "    model=model,\n",
    "    peft_config=lora_config,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    "    max_seq_length=max_seq_length,\n",
    "    packing=packing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 5: start training and save finetuned adapter weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save int4 model\n",
    "trainer.model.save_pretrained(output_dir, safe_serialization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear memory\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 6: merge adapter weights and base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load PEFT model in fp16\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    output_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,  # ATTENTION: This allows remote code execution\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge\n",
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save merged model\n",
    "merged_model.save_pretrained(save_model_dir, safe_serialization=True,  max_shard_size=\"2GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokenizer for easy inference\n",
    "tokenizer.save_pretrained(save_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del merged_model\n",
    "del tokenizer\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: restart the kernel and run from this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the test dataset and run all the cells within section 3: Create and prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inference: finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_local_path = \"/home/ubuntu/sft_cache/model/\"\n",
    "print(f\"model_local_path: {model_local_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_local_path, trust_remote_code=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_local_path,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_sample = dataset[6]\n",
    "eval_prompt, eval_completion = eval_sample[\"prompt\"], eval_sample[\"completion\"]\n",
    "\n",
    "print(f\"prompt: {eval_prompt}\")\n",
    "print(\"\\n\", f\"*\"*25, \"\\n\")\n",
    "print(f\"completion: {eval_completion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([eval_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "sft_model.eval()\n",
    "with torch.no_grad():\n",
    "    generated_ids = sft_model.generate(\n",
    "        **model_inputs, max_new_tokens=1000, do_sample=True\n",
    "    )\n",
    "    results = tokenizer.batch_decode(generated_ids)[0]\n",
    "    # prompt_length = model_inputs['input_ids'].shape[1]\n",
    "    # results = tokenizer.batch_decode(generated_ids[prompt_length:])[0]\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inference: original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sft_model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistral-community/Codestral-22B-v0.1\"\n",
    "print(f\"model_id: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_local_path, trust_remote_code=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_sample = dataset[6]\n",
    "eval_prompt, eval_completion = eval_sample[\"prompt\"], eval_sample[\"completion\"]\n",
    "\n",
    "print(f\"prompt: {eval_prompt}\")\n",
    "print(\"\\n\", f\"*\"*25, \"\\n\")\n",
    "print(f\"completion: {eval_completion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([eval_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "base_model.eval()\n",
    "with torch.no_grad():\n",
    "    generated_ids = base_model.generate(\n",
    "        **model_inputs, max_new_tokens=1000, do_sample=True\n",
    "    )\n",
    "    results = tokenizer.batch_decode(generated_ids)[0]\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inference: chat with vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install vllm ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistral-community/Codestral-22B-v0.1\"\n",
    "# model_id = \"/home/ubuntu/sft_cache/model/\"\n",
    "\n",
    "print(f\"model_id: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(model_id, tensor_parallel_size=4, dtype=\"bfloat16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"How to solve high leverage AI research problems ? And give examples where AI research helped humanity make leaps of progress.\"\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=1024)\n",
    "\n",
    "output = llm.generate(prompt, sampling_params)\n",
    "print(output[0].outputs[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
