{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup development environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Pytorch for FSDP and FA/SDPA\n",
    "%pip install --quiet \"torch==2.3.0\" tensorboard\n",
    " \n",
    "# Install Hugging Face libraries\n",
    "%pip install  --upgrade --quiet \\\n",
    "    \"transformers==4.40.0\" \"datasets==2.18.0\" \"accelerate==0.29.3\" \"evaluate==0.4.1\" \"bitsandbytes==0.43.1\" \"huggingface_hub==0.22.2\" \"trl==0.8.6\" \"peft==0.10.0\"\n",
    "  \n",
    "# Install flash-attn\n",
    "%pip install --quiet flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_dataset_local_path: /home/ubuntu/finetune-llms-on-aws/practise-fsdp/sft_cache/data\n"
     ]
    }
   ],
   "source": [
    "from utils import data_utils\n",
    "\n",
    "dataset_id = 'deepmind/code_contests'\n",
    "save_dataset_local_path = \"/home/ubuntu/finetune-llms-on-aws/practise-fsdp/sft_cache/data\"\n",
    "\n",
    "print(f\"save_dataset_local_path: {save_dataset_local_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07476e3a2fe142e69a27f94b65e1b49e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447d31803f2a4a0ba083874f49bace36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19121 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4381de1e9e49a2aea7ddf3616175d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59894ffb87354754b5e9e583a0a8380a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/487 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_dataset): 19121, len(test_dataset): 487\n"
     ]
    }
   ],
   "source": [
    "train_dataset = data_utils.load_and_process(\n",
    "    dataset_id=dataset_id, \n",
    "    split=\"train[:60%]\"\n",
    ")\n",
    "\n",
    "test_dataset = data_utils.load_and_process(\n",
    "    dataset_id=dataset_id,\n",
    "    split=\"test\"\n",
    ")\n",
    "\n",
    "print(f\"len(train_dataset): {len(train_dataset)}, len(test_dataset): {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0480d4fb884fd3a2be28ef2f546e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc9bed170c34f8cb4eb498f21624fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset files saved to: /home/ubuntu/finetune-llms-on-aws/practise-fsdp/sft_cache/data\n"
     ]
    }
   ],
   "source": [
    "train_dataset.to_json(\n",
    "    f\"{save_dataset_local_path}/train_dataset.json\", \n",
    "    orient=\"records\", \n",
    "    force_ascii=False\n",
    ")\n",
    "\n",
    "test_dataset.to_json(\n",
    "    f\"{save_dataset_local_path}/test_dataset.json\", \n",
    "    orient=\"records\", \n",
    "    force_ascii=False\n",
    ")\n",
    "\n",
    "print(f\"dataset files saved to: {save_dataset_local_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Set arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config/codestral_fsdp_qlora.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config/codestral_fsdp_qlora.yaml\n",
    "\n",
    "### training related\n",
    "dataset_path: \"/home/ubuntu/finetune-llms-on-aws/practise-fsdp/sft_cache/data\"\n",
    "output_dir: \"/home/ubuntu/finetune-llms-on-aws/practise-fsdp/sft_cache/checkpoints\" # prexisting folder path\n",
    "sm_save_model_dir: \"/home/ubuntu/finetune-llms-on-aws/practise-fsdp/sft_cache/model/\"  # prexisting folder path\n",
    "logging_dir: \"/home/ubuntu/finetune-llms-on-aws/practise-fsdp/sft_cache/checkpoints/logs\"\n",
    "\n",
    "model_id: \"mistral-community/Codestral-22B-v0.1\"\n",
    "num_train_epochs: 1\n",
    "max_steps: -1   # mumber of training steps (overrides num_train_epochs)\n",
    "per_device_train_batch_size: 1\n",
    "per_device_eval_batch_size: 1\n",
    "gradient_accumulation_steps: 1\n",
    "gradient_checkpointing: true\n",
    "gradient_checkpointing_kwargs: \n",
    "    use_reentrant: false\n",
    "bf16: true\n",
    "tf32: true\n",
    "max_grad_norm: 0.3\n",
    "weight_decay: 0.001\n",
    "optim: \"adamw_torch\"\n",
    "learning_rate: 0.0002\n",
    "warmup_ratio: 0.03\n",
    "lr_scheduler_type: \"constant\"\n",
    "save_strategy: \"no\"\n",
    "logging_steps: 25\n",
    "logging_strategy: \"steps\"\n",
    "group_by_length: true\n",
    "max_seq_length: 4096\n",
    "packing: false\n",
    "finetune_with_sm: false\n",
    "merge_weights_and_save: true\n",
    "save_tokenizer: true\n",
    "attn_implementation: \"sdpa\"\n",
    "\n",
    "### qlora related\n",
    "lora_r: 64\n",
    "lora_alpha: 16\n",
    "lora_dropout: 0.1\n",
    "task_type: \"CAUSAL_LM\"\n",
    "\n",
    "### bitsandbytes related\n",
    "load_in_4bit: true\n",
    "bnb_4bit_use_double_quant: true\n",
    "bnb_4bit_quant_type: \"nf4\"\n",
    "bnb_4bit_compute_dtype: \"bfloat16\"\n",
    "bnb_4bit_quant_storage: \"bfloat16\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Begin training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "Detected extra arguments that are going to be ignored: ['--nnodes=1', '--nproc-per-node=4'] - make sure to double check what you are doing\n",
      "ScriptArguments(dataset_path='/home/ubuntu/finetune-llms-on-aws/practise-fsdp/sft_cache/data', sm_save_model_dir='/home/ubuntu/finetune-llms-on-aws/practise-fsdp/sft_cache/model/', model_id='mistral-community/Codestral-22B-v0.1', max_seq_length=4096, packing=False, finetune_with_sm=False, merge_weights_and_save=True, save_tokenizer=True, attn_implementation='sdpa', lora_r=64, lora_alpha=16, lora_dropout=0.1, task_type='CAUSAL_LM', load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type='nf4', bnb_4bit_compute_dtype='bfloat16', bnb_4bit_quant_storage='bfloat16')\n",
      "Loading checkpoint shards:   0%|                          | 0/9 [00:00<?, ?it/s]^C\n",
      "W0618 17:22:15.306000 139754035836736 torch/distributed/elastic/agent/server/api.py:741] Received Signals.SIGINT death signal, shutting down workers\n",
      "W0618 17:22:15.306000 139754035836736 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 26783 closing signal SIGINT\n"
     ]
    }
   ],
   "source": [
    "! ACCELERATE_USE_FSDP=1 FSDP_CPU_RAM_EFFICIENT_LOADING=1 \\\n",
    "    torchrun scripts/sft_fsdp_qlora.py \\\n",
    "    --nnodes=1 --nproc-per-node=4 --config config/codestral_fsdp_qlora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_local_path = \"/home/ubuntu/finetune-llms-on-aws/practise-fsdp/sft_cache/model/\"\n",
    "print(f\"model_local_path: {model_local_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_local_path, trust_remote_code=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_local_path,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_sample = test_dataset[6]\n",
    "eval_prompt, eval_completion = eval_sample[\"messages\"][0][\"content\"], eval_sample[\"messages\"][2][\"content\"]\n",
    "\n",
    "print(f\"prompt: {eval_prompt}\")\n",
    "print(\"\\n\", f\"*\"*25, \"\\n\")\n",
    "print(f\"completion: {eval_completion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([eval_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "sft_model.eval()\n",
    "with torch.no_grad():\n",
    "    generated_ids = sft_model.generate(\n",
    "        **model_inputs, max_new_tokens=1000, do_sample=True\n",
    "    )\n",
    "    results = tokenizer.batch_decode(generated_ids)[0]\n",
    "    # prompt_length = model_inputs['input_ids'].shape[1]\n",
    "    # results = tokenizer.batch_decode(generated_ids[prompt_length:])[0]\n",
    "    print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
