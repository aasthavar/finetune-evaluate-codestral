{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup development environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Pytorch for FSDP and FA/SDPA\n",
    "%pip install --quiet \"torch==2.3.0\" tensorboard\n",
    " \n",
    "# Install Hugging Face libraries\n",
    "%pip install  --upgrade --quiet \\\n",
    "    \"transformers==4.40.0\" \"datasets==2.18.0\" \"accelerate==0.29.3\" \"evaluate==0.4.1\" \"bitsandbytes==0.43.1\" \"huggingface_hub==0.22.2\" \"trl==0.8.6\" \"peft==0.10.0\"\n",
    "  \n",
    "# Install flash-attn\n",
    "%pip install --quiet flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import data_utils\n",
    "\n",
    "dataset_id = 'deepmind/code_contests'\n",
    "save_dataset_local_path = \"/home/ubuntu/finetune-llms-on-aws/practise-fsdp/sft_cache/data\"\n",
    "\n",
    "print(f\"save_dataset_local_path: {save_dataset_local_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data_utils.load_and_process(\n",
    "    dataset_id=dataset_id, \n",
    "    split=\"train[:60%]\"\n",
    ")\n",
    "\n",
    "test_dataset = data_utils.load_and_process(\n",
    "    dataset_id=dataset_id,\n",
    "    split=\"test\"\n",
    ")\n",
    "\n",
    "print(f\"len(train_dataset): {len(train_dataset)}, len(test_dataset): {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.to_json(\n",
    "    f\"{save_dataset_local_path}/train_dataset.json\", \n",
    "    orient=\"records\", \n",
    "    force_ascii=False\n",
    ")\n",
    "\n",
    "test_dataset.to_json(\n",
    "    f\"{save_dataset_local_path}/test_dataset.json\", \n",
    "    orient=\"records\", \n",
    "    force_ascii=False\n",
    ")\n",
    "\n",
    "print(f\"dataset files saved to: {save_dataset_local_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Set arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config/codestral_fsdp_qlora.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config/codestral_fsdp_qlora.yaml\n",
    "\n",
    "### training related\n",
    "dataset_path: \"/home/ubuntu/finetune-llms-on-aws/practise-fsdp/sft_cache/data\" # prexisting folder path\n",
    "output_dir: \"/home/ubuntu/finetune-llms-on-aws/practise-fsdp/sft_cache/checkpoints\" # prexisting folder path\n",
    "sm_save_model_dir: \"/home/ubuntu/finetune-llms-on-aws/practise-fsdp/sft_cache/model/\"  # prexisting folder path\n",
    "logging_dir: \"/home/ubuntu/finetune-llms-on-aws/practise-fsdp/sft_cache/checkpoints/logs\" # prexisting folder path\n",
    "\n",
    "model_id: \"mistral-community/Codestral-22B-v0.1\"\n",
    "num_train_epochs: 1\n",
    "max_steps: -1 # mumber of training steps (overrides num_train_epochs)\n",
    "per_device_train_batch_size: 1\n",
    "per_device_eval_batch_size: 1\n",
    "gradient_accumulation_steps: 1\n",
    "gradient_checkpointing: true\n",
    "gradient_checkpointing_kwargs: \n",
    "    use_reentrant: false\n",
    "bf16: true\n",
    "tf32: true\n",
    "max_grad_norm: 0.3\n",
    "weight_decay: 0.001\n",
    "optim: \"adamw_torch\"\n",
    "learning_rate: 0.0002\n",
    "warmup_ratio: 0.03\n",
    "lr_scheduler_type: \"constant\"\n",
    "save_strategy: \"no\"\n",
    "logging_steps: 25\n",
    "logging_strategy: \"steps\"\n",
    "group_by_length: true\n",
    "max_seq_length: 4096\n",
    "packing: false\n",
    "finetune_with_sm: false\n",
    "merge_weights_and_save: true\n",
    "save_tokenizer: true\n",
    "attn_implementation: \"sdpa\"\n",
    "\n",
    "### qlora related\n",
    "lora_r: 64\n",
    "lora_alpha: 16\n",
    "lora_dropout: 0.1\n",
    "task_type: \"CAUSAL_LM\"\n",
    "\n",
    "### bitsandbytes related\n",
    "load_in_4bit: true\n",
    "bnb_4bit_use_double_quant: true\n",
    "bnb_4bit_quant_type: \"nf4\"\n",
    "bnb_4bit_compute_dtype: \"bfloat16\"\n",
    "bnb_4bit_quant_storage: \"bfloat16\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Begin training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ACCELERATE_USE_FSDP=1 FSDP_CPU_RAM_EFFICIENT_LOADING=1 \\\n",
    "    torchrun scripts/sft_fsdp_qlora.py \\\n",
    "    --nnodes=1 --nproc-per-node=4 --config config/codestral_fsdp_qlora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_local_path = \"/home/ubuntu/finetune-llms-on-aws/practise-fsdp/sft_cache/model/\"\n",
    "print(f\"model_local_path: {model_local_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_local_path, trust_remote_code=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_local_path,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_sample = test_dataset[6]\n",
    "eval_prompt, eval_completion = eval_sample[\"messages\"][0][\"content\"], eval_sample[\"messages\"][2][\"content\"]\n",
    "\n",
    "print(f\"prompt: {eval_prompt}\")\n",
    "print(\"\\n\", f\"*\"*25, \"\\n\")\n",
    "print(f\"completion: {eval_completion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([eval_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "sft_model.eval()\n",
    "with torch.no_grad():\n",
    "    generated_ids = sft_model.generate(\n",
    "        **model_inputs, max_new_tokens=1000, do_sample=True\n",
    "    )\n",
    "    results = tokenizer.batch_decode(generated_ids)[0]\n",
    "    # prompt_length = model_inputs['input_ids'].shape[1]\n",
    "    # results = tokenizer.batch_decode(generated_ids[prompt_length:])[0]\n",
    "    print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
