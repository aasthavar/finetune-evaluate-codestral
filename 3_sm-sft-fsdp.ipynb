{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup development environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade --quiet boto3 sagemaker huggingface datasets plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, boto3, sagemaker\n",
    "\n",
    "dataset_id = 'deepmind/code_contests'\n",
    "model_id = \"mistral-community/Codestral-22B-v0.1\"\n",
    "base_job_name = \"fsdp-codestral\"\n",
    "workspace_bucket_name = \"research-agi\"\n",
    "s3_prefix = \"mistral-community-codestral-22b-v0x1\"\n",
    "s3_train_dataset_path = f\"s3://{workspace_bucket_name}/{s3_prefix}/train\"\n",
    "s3_test_dataset_path = f\"s3://{workspace_bucket_name}/{s3_prefix}/test\"\n",
    "s3_save_model_dir = f\"s3://{workspace_bucket_name}/{s3_prefix}/runs/\"\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "session = sagemaker.session.Session(default_bucket=workspace_bucket_name)\n",
    "region = session._region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and save train dataset\n",
    "train_dataset = data_utils.load_and_process(\n",
    "    dataset_id=dataset_id,\n",
    "    split=\"train[:60%]\"\n",
    ")\n",
    "print(f\"train_dataset: {train_dataset}\")\n",
    "train_dataset.save_to_disk(s3_train_dataset_path)\n",
    "print(f\"s3_train_dataset_path: {s3_train_dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and save test dataset\n",
    "test_dataset = data_utils.load_and_process(\n",
    "    dataset_id=dataset_id,\n",
    "    split=\"test\"\n",
    ")\n",
    "print(f\"test_dataset: {test_dataset}\")\n",
    "test_dataset.save_to_disk(s3_test_dataset_path)\n",
    "print(f\"s3_test_dataset_path: {s3_test_dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot(tokenized_train_dataset, tokenized_test_dataset):\n",
    "    lengths = [len(x[\"input_ids\"]) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x[\"input_ids\"]) for x in tokenized_test_dataset]\n",
    "\n",
    "    fig = px.histogram(lengths)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "def template_dataset(examples):\n",
    "    return { \n",
    "            \"input_ids\": tokenizer.apply_chat_template(\n",
    "                                examples[\"messages\"], \n",
    "                                tokenize=False,\n",
    "                                # truncation=True,\n",
    "                                # max_length=4096, \n",
    "    )}\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(template_dataset, remove_columns=[\"messages\"])\n",
    "tokenized_test_dataset = test_dataset.map(template_dataset, remove_columns=[\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(tokenized_train_dataset, tokenized_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Set arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    ### training related\n",
    "    \"dataset_path\": \"/opt/ml/input/data\",\n",
    "    \"sm_save_model_dir\": \"/opt/ml/model\",\n",
    "    \"output_dir\":  \"/tmp\", \n",
    "    \"logging_dir\": \"/tmp/logs\",\n",
    "    \n",
    "    \"model_id\": \"mistral-community/Codestral-22B-v0.1\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"max_steps\": -1,\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"gradient_checkpointing_kwargs\": {\n",
    "        \"use_reentrant\": False,\n",
    "    },  \n",
    "    \"bf16\": True,\n",
    "    \"tf32\": True,\n",
    "    \"max_grad_norm\": 0.3,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"optim\": \"adamw_torch\",\n",
    "    \"learning_rate\": 0.0002,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"lr_scheduler_type\": \"constant\",\n",
    "    \"save_strategy\": \"no\",\n",
    "    \"logging_steps\": 25,\n",
    "    \"logging_strategy\": \"steps\",\n",
    "    \"group_by_length\": True,\n",
    "    \"max_seq_length\": 4096,\n",
    "    \"packing\": False,\n",
    "    \"finetune_with_sm\": True,\n",
    "    \"merge_weights_and_save\": True,\n",
    "    \"save_tokenizer\": True,\n",
    "    \"attn_implementation\": \"sdpa\",\n",
    "\n",
    "    ### qlora related\n",
    "    \"lora_r\": 64,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.1, \n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "\n",
    "    ### bitsandbytes related\n",
    "    \"load_in_4bit\": True,\n",
    "    \"bnb_4bit_use_double_quant\": True,\n",
    "    \"bnb_4bit_quant_type\": \"nf4\",\n",
    "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
    "    \"bnb_4bit_quant_storage\": \"bfloat16\", \n",
    "}\n",
    "\n",
    "print('Hyperparameters: \\n', json.dumps(hyperparameters, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Begin training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    source_dir                   = \"./scripts\",\n",
    "    entry_point                  = \"sft_fsdp_qlora.py\",\n",
    "    base_job_name                = base_job_name,\n",
    "    role                         = role,\n",
    "    sagemaker_session            = session,\n",
    "    framework_version            = \"2.3.0\",\n",
    "    py_version                   = \"py311\", \n",
    "    instance_count               = 1,\n",
    "    instance_type                = \"ml.p4d.24xlarge\", # gpus=8\n",
    "    volume_size                  = 300,\n",
    "    max_run                      = 1*24*60*60, # days * hours * minutes * seconds\n",
    "    hyperparameters              = hyperparameters,\n",
    "    disable_profiler             = True,\n",
    "    keep_alive_period_in_seconds = 1800,\n",
    "    debugger_hook_config         = False,\n",
    "    distribution                 = {\"torch_distributed\": {\"enabled\": True}}, # enable torchrun\n",
    "    environment                  = {\"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\"},\n",
    "    disable_output_compression   = True,\n",
    "    output_path                  = s3_save_model_dir,\n",
    ")\n",
    "\n",
    "data = {\n",
    "    'train': s3_train_dataset_path,\n",
    "    'test' : s3_test_dataset_path,\n",
    "}\n",
    "\n",
    "print(f\"training_image_uri: {estimator.training_image_uri()}\")\n",
    "print(f\"data: {json.dumps(data, indent=2, default=str)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"estimator.model_data: {estimator.model_data}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
